import subprocessimport requestsimport timeimport signalimport jsonimport platformimport osimport threadingimport randomfrom statistics import meanfrom collections import defaultdictfrom datetime import datetimeimport builtins as _biNAMESPACE = "gobgp-lab"TARGET_PORT = 8080LOCAL_BASE_PORT = 18080RUNS = 3OBSERVE_INTERVAL = float(os.getenv("OBSERVE_INTERVAL_S", "1.0"))CONNECT_TIMEOUT = float(os.getenv("CONNECT_TIMEOUT_S", "0.8"))READ_TIMEOUT = float(os.getenv("READ_TIMEOUT_S", "0.8"))PORTFWD_READY_RETRIES = int(os.getenv("PORTFWD_READY_RETRIES", "40"))PORTFWD_READY_SLEEP = float(os.getenv("PORTFWD_READY_SLEEP_S", "0.25"))UNINSTALL_WAIT = 10INSTALL_WAIT = 30POD_CHECK_INTERVAL = 2LONG_OP_TIMEOUTS = {    "/noise/start": float(os.getenv("NOISE_START_TIMEOUT_S", "30")),    "/noise/stop":  float(os.getenv("NOISE_STOP_TIMEOUT_S",  "180")),    "/pcap/start":  float(os.getenv("PCAP_START_TIMEOUT_S",  "20")),    "/pcap/stop":   float(os.getenv("PCAP_STOP_TIMEOUT_S",   "120")),}EU_TIME_FMT = "%d.%m.%Y %H:%M:%S"__orig_print = _bi.printdef print(*args, **kwargs):    __orig_print(f"[{datetime.now().strftime(EU_TIME_FMT)}]", *args, **kwargs)STABLE_SAMPLES = int(os.getenv("STABLE_SAMPLES", "3"))SEQUENCES = [    [        {"gate": "start_watchers"},        {"pod": "aachen", "method": "POST", "path": "/noise/start",         "json": {"PREFIX_BLOCK": 0, "NUMBER_OF_BLOCKS": 1, "rate": 1.0,                  "lifetime": 60, "jitter": 0.5, "max_active": 60}},        {"wait": 30},        {"pod": "aachen", "method": "POST", "path": "/noise/pause"},        {"wait": 30},        {"pod": "aachen", "method": "POST", "path": "/noise/drain", "json": {"percent": 50}},        {"wait": 30},        {"pod": "aachen", "method": "POST", "path": "/noise/stop"},        {"gate": "wait_for_drain"},    ],    [        {"gate": "start_watchers"},        {"pod": "aachen", "method": "POST", "path": "/noise/start",         "json": {"PREFIX_BLOCK": 0, "NUMBER_OF_BLOCKS": 2, "rate": 2.0,                  "lifetime": 45, "jitter": 0.2, "max_active": 90}},        {"wait": 20},        {"pod": "aachen", "method": "POST", "path": "/noise/stop"},        {"gate": "wait_for_drain"},    ]]GATES = {}def gate(name):    def wrapper(fn):        GATES[name] = fn        return fn    return wrapperdef get_running_pods():    cmd = ["kubectl", "get", "pods", "-n", NAMESPACE, "-o", "json"]    out = subprocess.check_output(cmd, text=True)    data = json.loads(out)    return [item["metadata"]["name"]            for item in data["items"]            if item["status"]["phase"] == "Running"]def resolve_pod_name(name_or_prefix: str) -> str:    if "-" in name_or_prefix:        return name_or_prefix    for name in get_running_pods():        if name.startswith(name_or_prefix):            return name    raise RuntimeError(f"No running pod found with prefix '{name_or_prefix}'")def port_forward(pod: str, local_port: int):    cmd = ["kubectl", "port-forward", f"pod/{pod}", f"{local_port}:{TARGET_PORT}", "-n", NAMESPACE]    return subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)def stop_process(proc):    if not proc:        return    if platform.system() == "Windows":        proc.terminate()    else:        proc.send_signal(signal.SIGINT)    try:        proc.wait(timeout=3)    except subprocess.TimeoutExpired:        proc.kill()def request_json(base_url, method, path, *, read_timeout=None, **kwargs):    rt = read_timeout if read_timeout is not None else READ_TIMEOUT    timeout = kwargs.pop("timeout", (CONNECT_TIMEOUT, rt))    func = getattr(requests, method.lower())    try:        r = func(base_url + path, timeout=timeout, **kwargs)    except requests.RequestException as e:        return 599, {"error": str(e)}    try:        return r.status_code, r.json()    except Exception:        return r.status_code, r.textdef wait_port_forward_ready(base_url):    for _ in range(PORTFWD_READY_RETRIES):        status, _ = request_json(base_url, "GET", "/ip", read_timeout=2.0)        if status == 200:            return True        time.sleep(PORTFWD_READY_SLEEP)    return Falsedef wait_for_pods_gone():    while True:        pods = get_running_pods()        if not pods:            return        time.sleep(POD_CHECK_INTERVAL)def wait_for_pods_ready():    while True:        pods = get_running_pods()        if pods:            return        time.sleep(POD_CHECK_INTERVAL)def reset_environment(opera_enabled, opera_mode="bitfield"):    print("[Setup] uninstalling previous release...")    subprocess.run(["helm", "uninstall", "gobgp-lab", "-n", NAMESPACE], check=False)    wait_for_pods_gone()    print(f"[Setup] waiting {UNINSTALL_WAIT}s after uninstall...")    time.sleep(UNINSTALL_WAIT)    print("[Setup] installing fresh release...")    subprocess.run([        "helm", "upgrade", "--install",        "gobgp-lab", "gobgp-lab",        "-n", NAMESPACE, "--create-namespace",        "--set", f"opera.enabled={str(opera_enabled).lower()}",        "--set", f"opera.mode={opera_mode}"    ], check=True)    wait_for_pods_ready()    print(f"[Setup] waiting {INSTALL_WAIT}s for stabilization...")    time.sleep(INSTALL_WAIT)def rib_observer(pod_name, base_url, results_dict, stop_event):    print(f"[Watcher] starting for {pod_name}")    series = []    start_ts = time.time()    stable_count = 0    saw_nonzero = False    drained = False    first_drained_ts = None    results_dict[pod_name] = {"series": [], "drained": False, "saw_nonzero": False,                              "time_to_zero": None, "last_sample_ts": None, "reason": "watching"}    time.sleep(random.uniform(0, OBSERVE_INTERVAL))    while not stop_event.is_set():        status_sum, rib_sum = request_json(base_url, "GET", "/rib/summary", read_timeout=max(READ_TIMEOUT, 2.0))        status_len, rib_len = request_json(base_url, "GET", "/rib/pathlengths", read_timeout=max(READ_TIMEOUT, 2.0))        now = time.time()        if status_sum == 200 and isinstance(rib_sum, dict):            num_paths = int(rib_sum.get("num_paths", 0))            num_dests = int(rib_sum.get("num_destinations", 0))            min_len = rib_len.get("min", 0) if status_len == 200 and isinstance(rib_len, dict) else None            avg_len = rib_len.get("avg", 0.0) if status_len == 200 and isinstance(rib_len, dict) else None            max_len = rib_len.get("max", 0) if status_len == 200 and isinstance(rib_len, dict) else None            series.append({                "ts": now,                "ts_local": datetime.fromtimestamp(now).strftime(EU_TIME_FMT),                "num_paths": num_paths,                "num_destinations": num_dests,                "path_len_min": min_len,                "path_len_avg": avg_len,                "path_len_max": max_len            })            if num_paths > 0:                saw_nonzero = True; stable_count = 0; drained = False            else:                if saw_nonzero:                    stable_count += 1                    if not drained and stable_count >= STABLE_SAMPLES:                        drained = True; first_drained_ts = first_drained_ts or now            results_dict[pod_name] = {"series": series, "drained": drained, "saw_nonzero": saw_nonzero,                                      "time_to_zero": (first_drained_ts - start_ts) if first_drained_ts else None,                                      "last_sample_ts": now, "reason": "watching"}        time.sleep(OBSERVE_INTERVAL)    results_dict[pod_name] = {"series": series, "drained": drained, "saw_nonzero": saw_nonzero,                              "time_to_zero": (first_drained_ts - start_ts) if first_drained_ts else None,                              "last_sample_ts": time.time(), "reason": "stopped_by_gate"}    print(f"[Watcher] finished for {pod_name} drained={drained}")def connect_if_needed(pod_connections, name_or_prefix):    pod_name = resolve_pod_name(name_or_prefix)    key = pod_name    if key not in pod_connections:        local_port = LOCAL_BASE_PORT + len(pod_connections)        print(f"[Main] port-forward {pod_name} on {local_port}")        proc = port_forward(pod_name, local_port)        base = f"http://localhost:{local_port}"        if not wait_port_forward_ready(base):            stop_process(proc)            raise RuntimeError(f"port-forward not ready for {pod_name} on {local_port}")        pod_connections[key] = (proc, pod_name, local_port)    return pod_connections[key]@gate("start_watchers")def start_watchers_gate(_base_url, step, results, state):    print("[Gate] start_watchers")    pods = get_running_pods()    watch = {"threads": [], "stop_event": threading.Event(), "results": {}, "watched_pods": []}    state["watch"] = watch    for full_name in pods:        if full_name.startswith("aachen"):            continue        proc, pod_name, local_port = connect_if_needed(state["pod_connections"], full_name)        base = f"http://localhost:{local_port}"        t = threading.Thread(target=rib_observer, args=(pod_name, base, watch["results"], watch["stop_event"]), daemon=True)        t.start()        watch["threads"].append(t)        watch["watched_pods"].append(pod_name)    return {"gate": "start_watchers", "watching": state["watch"]["watched_pods"]}@gate("wait_for_drain")def wait_for_drain_gate(_base_url, step, results, state):    print("[Gate] wait_for_drain")    all_stable_rounds = max(1, STABLE_SAMPLES)    stable_count = 0    while True:        statuses = [bool(state["watch"]["results"].get(p, {}).get("drained")) for p in state["watch"]["watched_pods"]]        if all(statuses):            stable_count += 1        else:            stable_count = 0        if stable_count >= all_stable_rounds:            print("[Gate] all watchers drained")            break        time.sleep(OBSERVE_INTERVAL)    state["watch"]["stop_event"].set()    for t in state["watch"]["threads"]:        t.join(timeout=max(5.0, 5 * OBSERVE_INTERVAL))    return {"gate": "wait_for_drain", "drained": True, "stable_rounds": all_stable_rounds}def run_step(base_url, step, pod_name, exp_dir):    url = base_url + step["path"]    method = step["method"].lower()    kwargs = {}    if "json" in step:        kwargs["json"] = step["json"]    read_timeout = max(LONG_OP_TIMEOUTS.get(step["path"], READ_TIMEOUT), READ_TIMEOUT)    print(f"[Main] {pod_name} {step['method']} {step['path']}")    action_entry = {"pod": pod_name, "method": step["method"].upper(), "path": step["path"]}    try:        resp = requests.request(method, url, timeout=(CONNECT_TIMEOUT, read_timeout), **kwargs)        try:            parsed = resp.json()        except Exception:            parsed = resp.text        action_entry["result"] = {"status": resp.status_code, "response": parsed}    except requests.RequestException as e:        action_entry["result"] = {"status": "ERROR", "response": str(e)}        return {"action": action_entry}    return {"action": action_entry}def run_experiment(run_id, mode, base_dir, opera_enabled, sequence):    reset_environment(opera_enabled)    exp_dir = os.path.join(base_dir, str(run_id))    os.makedirs(exp_dir, exist_ok=True)    results = []    pod_connections = {}    state = {"pod_connections": pod_connections}    try:        for step in sequence:            if "wait" in step:                time.sleep(step["wait"]); results.append({"wait": step["wait"]}); continue            if "gate" in step:                fn = GATES.get(step["gate"])                gate_result = fn(None, step, results, state) if fn else {"gate": step["gate"], "error": "unknown gate"}                results.append(gate_result); continue            proc, pod_name, local_port = connect_if_needed(pod_connections, step["pod"])            base_url = f"http://localhost:{local_port}"            results.append(run_step(base_url, step, pod_name, exp_dir))    finally:        if "watch" in state: state["observers"] = state["watch"]["results"]        for proc, _, _ in pod_connections.values(): stop_process(proc)    report = {"generated": datetime.now().isoformat(), "namespace": NAMESPACE,              "experiment_dir": exp_dir, "sequence": results, "observers": state.get("observers", {}),              "drain_parameters": {"stable_samples": STABLE_SAMPLES}}    with open(os.path.join(exp_dir, f"report_{mode}.json"), "w") as f: json.dump(report, f, indent=2)    return reportdef normalize_name(pod_name: str) -> str:    return pod_name.split("-")[0]def aggregate_series(reports):    agg = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))    for rep in reports:        for pod, obs in rep.get("observers", {}).items():            base = normalize_name(pod)            series = obs.get("series", [])            started = False            idx = 0            for s in series:                if not started:                    if s.get("num_paths", 0) > 0:                        started = True                    else:                        continue                if "num_paths" in s and s["num_paths"] is not None:                    agg[base][idx]["num_paths"].append(s["num_paths"])                if "num_destinations" in s and s["num_destinations"] is not None:                    agg[base][idx]["num_destinations"].append(s["num_destinations"])                if "path_len_min" in s and s["path_len_min"] is not None:                    agg[base][idx]["path_len_min"].append(s["path_len_min"])                if "path_len_avg" in s and s["path_len_avg"] is not None:                    agg[base][idx]["path_len_avg"].append(s["path_len_avg"])                if "path_len_max" in s and s["path_len_max"] is not None:                    agg[base][idx]["path_len_max"].append(s["path_len_max"])                idx += 1    summary = {}    for pod, by_index in agg.items():        summary[pod] = {}        for idx, metrics in sorted(by_index.items()):            summary[pod][idx] = {}            for metric, values in metrics.items():                if values:                    summary[pod][idx][metric] = {                        "min": min(values),                        "max": max(values),                        "avg": mean(values)                    }    return summarydef main():    for mode, opera_enabled in [("obgp", True), ("bgp", False)]:        for seq_idx, sequence in enumerate(SEQUENCES, start=1):            timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")            base_dir = os.path.join("experiments", f"{mode}-seq{seq_idx}-{timestamp}")            os.makedirs(base_dir, exist_ok=True)            reports = []            for i in range(1, RUNS + 1):                print(f"[Main] Mode={mode}, Seq={seq_idx} Starting run {i}/{RUNS}")                reports.append(run_experiment(i, mode, base_dir, opera_enabled, sequence))            per_pod_series = aggregate_series(reports)            summary = {"runs": RUNS, "mode": mode, "sequence": seq_idx, "per_pod_series": per_pod_series}            with open(os.path.join(base_dir, f"summary_{mode}.json"), "w") as f:                json.dump(summary, f, indent=2)            print(f"[Main] Summary for {mode} seq{seq_idx} saved under {base_dir}")if __name__ == "__main__":    main()